{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSI4142 - Data Science\n",
    "\n",
    "## Assignment 3 - Predictive analysis - Regression and Classification\n",
    "\n",
    "Shacha Parker 300235525\n",
    "\n",
    "Callum Frodsham 300199446\n",
    "\n",
    "This part of the assignment takes a dataset and performs an empirical study with a linear regression task.\n",
    "\n",
    "#### Execution of the Notebook\n",
    "\n",
    "Execution of the notebook requires a recent version of Python and a virtual environment if your operating system requires it.\n",
    "\n",
    "You also need the following libraries installed for import dependencies: numpy, pandas, seaborn, matplotlib, and sklearn.\n",
    "### Dataset Information\n",
    "Dataset name: <a href=\"https://www.kaggle.com/datasets/mirichoi0218/insurance\">Medical Cost Personal Datasets</a>\n",
    "<br>\n",
    "Provider: Miri Choi on Kaggle\n",
    "\n",
    "<b>Features</b>\n",
    "\n",
    "    age: age of primary beneficiary - quantitative - continuous\n",
    "\n",
    "    sex: insurance contractor gender, female, male - categorical, ordinal\n",
    "\n",
    "    bmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9 - quantitative - continuous\n",
    "\n",
    "    children: Number of children covered by health insurance / Number of dependents - quantitative, discrete\n",
    "\n",
    "    smoker: does the beneficiary smoke - categorical, nominal\n",
    "\n",
    "    region: the beneficiary's residential area in the US: northeast, southeast, southwest, northwest. - categorical, nominal\n",
    "\n",
    "    charges: Individual medical costs billed by health insurance - quantitative, continuous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "# CHANGE THIS TO GITHUB RAW\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/ShackuOttawa/Assignment3CSI4142/refs/heads/main/insurance.csv\")\n",
    "\n",
    "# output settings for debugging\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "The dataset is already clean. This can be seen with the results of the code cell below: no entries are null, and all values are appropriate. No cleaning is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.info())\n",
    "print(data.describe())\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing values in each column:\\n\", missing_values)\n",
    "\n",
    "# Display unique values\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "for column in categorical_columns:\n",
    "    print(f\"\\nUnique values in column '{column}':\\n\", data[column].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Feature Encoding\n",
    "Through the get_dummies function, we can create the dataset with all categorical features becoming one-hot vectors. The rest of the features remain as they were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the specified categorical features\n",
    "data = pd.get_dummies(data, columns=['sex', 'smoker', 'region'], drop_first=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA and Outlier detection\n",
    "a. Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison graph for sex_male against sex_female\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(data=data, x='sex_male')\n",
    "plt.title('Count of Male vs Female')\n",
    "plt.xlabel('Sex (0: Female, 1: Male)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Comparison graph for smoker counts\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(data=data, x='smoker_yes')\n",
    "plt.title('Count of Smokers vs Non-Smokers')\n",
    "plt.xlabel('Smoker (0: No, 1: Yes)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Comparison for regions\n",
    "plt.figure(figsize=(10, 5))\n",
    "region_columns = ['region_northeast', 'region_northwest', 'region_southeast', 'region_southwest']\n",
    "region_counts = data[region_columns].sum()\n",
    "sns.barplot(x=region_counts.index, y=region_counts.values)\n",
    "plt.title('Count of Regions')\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, outliers don't occur on categorical data like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all numerical features\n",
    "numerical_columns = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "for column in numerical_columns:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.boxplot(data=data, x=column)\n",
    "    plt.title(f'Box plot of {column}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since feature 'bmi' has only 9 outliers out of the 1000+ entries (0.67%), we can safely remove them without there being a significant statistical impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_outlier_removal = data.copy()\n",
    "original = data.shape\n",
    "\n",
    "Q1 = data['bmi'].quantile(0.25)\n",
    "Q3 = data['bmi'].quantile(0.75)\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out the outliers\n",
    "data = data[(data['bmi'] >= lower_bound) & (data['bmi'] <= upper_bound)]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(data=data, x=data['bmi'])\n",
    "plt.title(f'Box plot of BMI after outlier removal')\n",
    "plt.show()\n",
    "\n",
    "print(\"Original data shape:\", original)\n",
    "print(\"Data shape after removing outliers:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 'charges' outliers, it would negatively impact the dataset if all rows with 'charges' outliers were removed. Instead, we'll use  linear regression imputation to apply known values over the existing outlier values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D2 = data.copy()\n",
    "\n",
    "# Create LocalOutlierFactor, fit it to temporary dataframe D2, and remove outliers\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
    "D2['charges_outlier'] = lof.fit_predict(D2[['charges']])\n",
    "\n",
    "D2.loc[D2['charges_outlier'] == -1, 'charges'] = np.nan\n",
    "D2.drop(columns=['charges_outlier'], inplace=True)\n",
    "\n",
    "print(\"Number of NaN values in 'charges' after outlier removal:\", D2['charges'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This imputation strategy is also a demonstration of my knowledge and exploration of sklearn's LinearRegression method for part d) Predictive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features and target variable in the original data\n",
    "X_original = data.drop(columns=['charges'])\n",
    "y_original = data['charges']\n",
    "\n",
    "# Train a linear regression model on the original data\n",
    "imputation_model = LinearRegression()\n",
    "imputation_model.fit(X_original, y_original)\n",
    "\n",
    "# Identify the rows in D2 with missing 'charges' values\n",
    "missing_indices = D2[D2['charges'].isnull()].index\n",
    "X_missing = D2.loc[missing_indices].drop(columns=['charges'])\n",
    "\n",
    "# Replace missing values through imputation prediction\n",
    "imputed_values = imputation_model.predict(X_missing)\n",
    "D2.loc[missing_indices, 'charges'] = imputed_values\n",
    "\n",
    "missing_values_d2_after_imputation = D2['charges'].isnull().sum()\n",
    "print(\"Missing values in 'charges' column of D2 after imputation:\", missing_values_d2_after_imputation)\n",
    "\n",
    "# Return D2 values to main dataframe\n",
    "data = D2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "Before we proceed with feature engineering, we'll create new copies of the dataset with and without outlier removal that will not have the extra features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_outlier_removal_or_FE = data_without_outlier_removal.copy()\n",
    "data_without_FE = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first new feature, we will create the age-bmi column. The age-bmi value is a row's age multiplied by its bmi, which is then normalized by dividing this number by the median age times the median bmi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medianage = data['age'].median()\n",
    "medianbmi = data['bmi'].median()\n",
    "data['age-bmi'] = (data['age'] * data['bmi']) / (medianage * medianbmi)\n",
    "data_without_outlier_removal_with_FE = data_without_outlier_removal\n",
    "data_without_outlier_removal_with_FE['age-bmi'] = (data_without_outlier_removal['age'] * data_without_outlier_removal['bmi']) / (medianage * medianbmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we'll create a feature called \"childandsmoker\" which is a boolean value that is true if the row has a positive number of children and 'smoker' is 'yes'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['childandsmoker'] = np.where((data['smoker_yes'] == True) & (data['children'] > 0), True, False)\n",
    "data_without_outlier_removal_with_FE['childandsmoker'] = np.where((data_without_outlier_removal_with_FE['smoker_yes'] == True) & (data_without_outlier_removal_with_FE['children'] > 0), True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming these variables to be more legible\n",
    "data_with_both = data\n",
    "data_with_neither = data_without_outlier_removal_or_FE\n",
    "data_with_FE = data_without_outlier_removal_with_FE\n",
    "data_with_OR = data_without_FE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have:\n",
    "\n",
    "*data_with_both*: the data with both outliers removed and new features engineered.\n",
    "\n",
    "*data_with_neither*: data with neither outliers removed nor feature engineering\n",
    "\n",
    "*data_with_FE*: data including outliers without feature engineering applied\n",
    "\n",
    "*data_with_OR*: data with outliers removed but without feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we split each dataset into train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data_with_both dataset\n",
    "data_with_both_train, data_with_both_temp = train_test_split(data_with_both, test_size=0.3, random_state=0)\n",
    "data_with_both_validation, data_with_both_test = train_test_split(data_with_both_temp, test_size=0.5, random_state=0)\n",
    "\n",
    "# Split the data_with_neither dataset\n",
    "data_with_neither_train, data_with_neither_temp = train_test_split(data_with_neither, test_size=0.3, random_state=0)\n",
    "data_with_neither_validation, data_with_neither_test = train_test_split(data_with_neither_temp, test_size=0.5, random_state=0)\n",
    "\n",
    "# Split the data_with_FE dataset\n",
    "data_with_FE_train, data_with_FE_temp = train_test_split(data_with_FE, test_size=0.3, random_state=0)\n",
    "data_with_FE_validation, data_with_FE_test = train_test_split(data_with_FE_temp, test_size=0.5, random_state=0)\n",
    "\n",
    "# Split the data_with_OR dataset\n",
    "data_with_OR_train, data_with_OR_temp = train_test_split(data_with_OR, test_size=0.3, random_state=0)\n",
    "data_with_OR_validation, data_with_OR_test = train_test_split(data_with_OR_temp, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that \n",
    "def evaluate_model(X_train, y_train, X_val, y_val, info):\n",
    "    # Initialize the linear regression model\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Perform 4-fold cross-validation on the training set\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=0)\n",
    "    mse_scores = cross_val_score(model, X_train, y_train, cv=kf, scoring='neg_mean_squared_error')\n",
    "    r2_scores = cross_val_score(model, X_train, y_train, cv=kf, scoring='r2')\n",
    "\n",
    "    # Fit and then train the model on the designated train set\n",
    "    model.fit(X_train, y_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # Calculate MSE and R2 on the validation set\n",
    "    mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "    r2_val = r2_score(y_val, y_val_pred)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Results for\", info)\n",
    "    print(\"Cross-validated MSE scores:\", -mse_scores)\n",
    "    print(\"Cross-validated R2 scores:\", r2_scores)\n",
    "    print(\"Mean cross-validated MSE:\", -mse_scores.mean())\n",
    "    print(\"Mean cross-validated R2:\", r2_scores.mean())\n",
    "    print(\"Validation MSE:\", mse_val)\n",
    "    print(\"Validation R2:\", r2_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_with_neither_train.drop(columns=['charges'])\n",
    "y_train = data_with_neither_train['charges']\n",
    "X_val = data_with_neither_validation.drop(columns=['charges'])\n",
    "y_val = data_with_neither_validation['charges']\n",
    "\n",
    "evaluate_model(X_train, y_train, X_val, y_val, \"data_with_neither\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_with_OR_train.drop(columns=['charges'])\n",
    "y_train = data_with_OR_train['charges']\n",
    "X_val = data_with_OR_validation.drop(columns=['charges'])\n",
    "y_val = data_with_OR_validation['charges']\n",
    "\n",
    "evaluate_model(X_train, y_train, X_val, y_val, \"data_with_OR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_with_FE_train.drop(columns=['charges'])\n",
    "y_train = data_with_FE_train['charges']\n",
    "X_val = data_with_FE_validation.drop(columns=['charges'])\n",
    "y_val = data_with_FE_validation['charges']\n",
    "\n",
    "evaluate_model(X_train, y_train, X_val, y_val, \"data_with_FE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_with_both_train.drop(columns=['charges'])\n",
    "y_train = data_with_both_train['charges']\n",
    "X_val = data_with_both_validation.drop(columns=['charges'])\n",
    "y_val = data_with_both_validation['charges']\n",
    "\n",
    "evaluate_model(X_train, y_train, X_val, y_val, \"data_with_both\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the best performance is with the data with neither OR or FE applied, by a very small margin. We will use this now to run the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_with_neither_train.drop(columns=['charges'])\n",
    "y_train = data_with_neither_train['charges']\n",
    "# Prepare the test data for data_with_neither\n",
    "X_test_neither = data_with_neither_test.drop(columns=['charges'])\n",
    "y_test_neither = data_with_neither_test['charges']\n",
    "\n",
    "# Evaluate the model on the test dataset for data_with_neither\n",
    "evaluate_model(X_train, y_train, X_test_neither, y_test_neither, \"data_with_neither (Test Set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run the tests on the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_with_FE_train.drop(columns=['charges'])\n",
    "y_train = data_with_FE_train['charges']\n",
    "X_test = data_with_FE_test.drop(columns=['charges'])\n",
    "y_test = data_with_FE_test['charges']\n",
    "\n",
    "# Evaluate the model on the test dataset for data_with_FE\n",
    "evaluate_model(X_train, y_train, X_test, y_test, \"data_with_FE (Test Set)\")\n",
    "print()\n",
    "\n",
    "X_train = data_with_OR_train.drop(columns=['charges'])\n",
    "y_train = data_with_OR_train['charges']\n",
    "X_test_OR = data_with_OR_test.drop(columns=['charges'])\n",
    "y_test_OR = data_with_OR_test['charges']\n",
    "\n",
    "# Evaluate the model on the test dataset for data_with_OR\n",
    "evaluate_model(X_train, y_train, X_test_OR, y_test_OR, \"data_with_OR (Test Set)\")\n",
    "print()\n",
    "\n",
    "X_train = data_with_both_train.drop(columns=['charges'])\n",
    "y_train = data_with_both_train['charges']\n",
    "X_test_both = data_with_both_test.drop(columns=['charges'])\n",
    "y_test_both = data_with_both_test['charges']\n",
    "\n",
    "# Evaluate the model on the test dataset for data_with_both\n",
    "evaluate_model(X_train, y_train, X_test_both, y_test_both, \"data_with_both (Test Set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across the different dataset variations, the dataset with neither outlier removal nor feature engineering did indeed turn out to have the best scores (lowest MSE, highest R2) of the 4. It's worth noting that the difference in some scores was quite marginal. I tried the train_test_split again with a random value of 42 for all splits, and the feature engineering dataset was slightly higher, but almost in a near tie with the data_with_neither dataset in training/validation. However, when applied to testing, it turned out to be unexpectedly worse in scores than the other datasets. In my tests, I believe that outlier detection and removal generally worsened results slightly, while feature engineering had a neutral-to-positive effect, making it about equal to the dataset with neither technique applied. Interestingly, I found that in many tests, applying both techniques (data_with_both) consistently gave the worst scores of all. In the currently-set state, the results of the unseen tests lined up as expected with the cross-validation training/validations. I find this will typically be the case, but because the scores are so close to each other, it's possible some certain random configurations may cause unexpected results between the validation and the unseen tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
