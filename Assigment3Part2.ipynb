{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSI 4142 - Introduction to Data Science\n",
    "# Assignment 3: Predictive analysis Regression and Classification\n",
    "\n",
    "Shacha Parker (300235525)\\\n",
    "Callum Frodsham and (300199446)\\\n",
    "Group 79\n",
    "\n",
    "### Setup Instructions To Reproduce this Data Cleaning Notebook:\n",
    "(Step 1 Optional)\n",
    "1. Create a virtual python environment in the project directory (if you want) for all of the packages required:  \n",
    "``` \n",
    "python -m venv .venv\n",
    "```\n",
    "To enter the virutal environment: \n",
    "```\n",
    ".venv/Scripts/activate.ps1 # on windows\n",
    "source .venv/bin/activate # on mac/linux\n",
    "```\n",
    "2. Download all of the required packages (run in cmd/shell of choice):\n",
    "```\n",
    "pip install jupyter\n",
    "pip install ipykernel\n",
    "pip install pandas\n",
    "pip install numpy\n",
    "```\n",
    "3. VSCode: Ensure you have the correct python kernel selected!\n",
    "<br> \n",
    "If you are using a virtual environment, make sure to select the python interpreter for that virtual environment otherwise this will not work! If you have everything done globally, then just make sure the correct python kernel you are using is selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dataset 2: Breast Cancer</h1>\n",
    "<h3>Decision Tree Classification</h3>\n",
    "\n",
    "Author: Reihaneh Namdari\n",
    "<br>\n",
    "Purpose: The purpose of this dataset is to provide population-based cancer statistics on patients with infiltrating duct and lobular carcinoma breast cancer that were diagnosed in 2006-2010. \n",
    "<br>\n",
    "Shape: This dataset is composed of 16 columns, and 4024 rows.\n",
    "<br><br>\n",
    "Link: <a href=\"https://www.kaggle.com/datasets/reihanenamdari/breast-cancer\"> Breast Cancer Dataset</a>\n",
    "<br>\n",
    "Note: This description only includes 10/16 features, as the rest will not be used.\\\n",
    "The differentiate feature was removed because it is the same as the grade feature.\\\n",
    "The 6th Stage feature was removed because it can be derived by the n stage, t stage and grade features.\n",
    "\n",
    "<h3>Dataset Feature List: </h3>\n",
    "<ol>\n",
    "    <li>Age:\n",
    "    <br>\n",
    "    Feature Type: Numerical - Discrete\n",
    "    <br>\n",
    "    Description: The age in years of the patient.\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>Race:\n",
    "    <br>\n",
    "    Feature Type: Categorical - Nominal\n",
    "    <br>\n",
    "    Description: The race of patient.\n",
    "        </li>\n",
    "    <br>\n",
    "    <li>Marital Status:\n",
    "    <br>\n",
    "    Feature Type: Categorical - Nominal\n",
    "    <br>\n",
    "    Description: The marital status of the patient.\n",
    "        </li>\n",
    "    <br>\n",
    "    <li>T Stage:\n",
    "    <br>\n",
    "    Feature Type: Categorical - Ordinal\n",
    "    <br>\n",
    "    Description: The T stage refers to the Tumor stage of the TNM staging system that describes the extent of the cancer such as tumor size, and tumor invasion into nearby structures. T1-T3 in increasing severity. \n",
    "        </li>\n",
    "    <br>\n",
    "    <li>N Stage:\n",
    "    <br>\n",
    "    Feature Type: Categorical - Ordinal\n",
    "    <br>\n",
    "    Description: N Stage refers to the Node stage of the TNM staging system that describes whether the cancer has spread to other nearby lymph nodes, with N1-N3 increasing in severity.\n",
    "        </li>\n",
    "    <br>\n",
    "    <li>6th Stage:\n",
    "    <br>\n",
    "    Feature Type: Categorical - Ordinal\n",
    "    <br>\n",
    "    Description: Refers to the Breast Adjusted AJCC 6th Stage variables describing the extent of the disease (EOD), and the collaborative stage (CS). \n",
    "        </li>\n",
    "    <br>\n",
    "    <li>Grade:\n",
    "    <br>\n",
    "    Feature Type: Categorical - Ordinal\n",
    "    <br>\n",
    "    Description: The 4 grades, 1, 2, 3 and anaplastic Grade 4 describe the histologic grade of the cancer cells.\n",
    "        </li>\n",
    "    <br>\n",
    "    <li>A Stage:\n",
    "    <br>\n",
    "    Feature Type: Categorical - Nominal\n",
    "    <br>\n",
    "    Description: Two categories, distant and regional. Distant means that the tumor has spread/metastasized to far away organs/regions from the original site. Regional implies that that the tumor has extended in areas close to the original site. \n",
    "        </li>\n",
    "    <br>\n",
    "    <li>Tumor Size:\n",
    "    <br>\n",
    "    Feature Type: Numerical - Continuous\n",
    "    <br>\n",
    "    Description: The size of the tumor in exact millimeters.\n",
    "        </li>\n",
    "    <br>\n",
    "    <li>Survival Months:\n",
    "    <br>\n",
    "    Feature Type: Numerical - Continuous\n",
    "    <br>\n",
    "    Description: The length of time in months until the patients death or their last follow up.\n",
    "        </li>\n",
    "    <br>\n",
    "    <li>Status:\n",
    "    <br>\n",
    "    Feature Type: Categorical - Nominal\n",
    "    <br>\n",
    "    Description: The status of the patient at their last follow up, with two categories, dead or alive.\n",
    "        </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Imports:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import colormaps\n",
    "\n",
    "# Then load the dataset\n",
    "dataset = pd.read_csv(\"Breast_Cancer.csv\")\n",
    "# a list of excluded features to prevent the scope of this assignment from creeping. \n",
    "excluded_columns = ['Estrogen Status', 'Progesterone Status', 'Regional Node Examined', 'Reginol Node Positive', 'differentiate', '6th Stage']\n",
    "dataset = dataset.drop(columns=excluded_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "The Data Cleaning step will identify whether the dataset has any incorrect or missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about the dataframe\n",
    "print(dataset.info())\n",
    "\n",
    "# Describe the dataframe\n",
    "print(dataset.describe())\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = dataset.isnull().sum()\n",
    "print(\"Missing values in each column:\\n\", missing_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the data above, the data contains no null values, and each feature has the correct value types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for correct value range for each categorical feature:\n",
    "categorical_columns_initial = dataset.select_dtypes(include=['object']).columns\n",
    "for column in categorical_columns_initial:\n",
    "    print(f\"\\nUnique values in column '{column}':\\n\", dataset[column].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the data above, each categorical feature has the values and there are no incorrect or missing values. Therefore it can be ascertained that the dataset is \"clean\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting:\n",
    "We're going to divide this dataset into a test set, DT, and the reduced training set D = D - DT.\n",
    "We will then have 4 sets in total in the end:\\\n",
    "DR1 = DR\\\n",
    "DR2 = DR - outliers\\\n",
    "DR3 = DR + aggregation\\\n",
    "DR4 = (DR + aggregation) - outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create the test set, DT, we're going to divide the dataset in 80/20, and thus the 20 leftover will be the test set.\n",
    "DR, DT = train_test_split(dataset, test_size=0.2, random_state=23)\n",
    "# confirm they are the correct size\n",
    "print(f\"Number of rows in DR: {DR.shape[0]}\")\n",
    "print(f\"Number of rows in DT: {DT.shape[0]}\")\n",
    "ratio = (DT.shape[0] / (DR.shape[0] + DT.shape[0]))\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA and Outlier detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Feature Exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating visualizations from the reduced training categorical columns:\n",
    "new_categorical_columns = DR.select_dtypes(include=['object']).columns\n",
    "color_list = list(colormaps)\n",
    "count = 0;\n",
    "for column in new_categorical_columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    graph = sns.countplot(data=DR, x=column, hue=column, palette=color_list[count])\n",
    "    count += 3\n",
    "    for bar in graph.containers:\n",
    "        graph.bar_label(bar)\n",
    "    plt.title(f\"{column.title()} Count Plot\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Outliers for Categorical Features:\n",
    "\n",
    "Most of the categorical features do not contain any outliers, however the marital status feature does contain an outlying category, specfically the \"separated\" category. The separated category only has 30/3219 or approximately 0.9319% of the values, and is also similar to another category, divorced. The distinction between separated and divorced should not matter as much, and thus both category will be combined into one. \\\n",
    "The \"Grade\", and \"A Stage\" features have their own categories that are much rarer than the others. Those being, anaplastic, and distant. These rare categories will not be considered outliers as they are important, and could provide meaningful insight later on in the empirical study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA of Numerical Features and Outlier Detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all of the numerical columns\n",
    "numerical_data = DR.select_dtypes(include=['int64'])\n",
    "\n",
    "# create a box plot of the age to view any possible outliers\n",
    "sns.boxplot(data=DR, x='Age')\n",
    "plt.title(\"Age Box Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the Age Box plot above, there are no outliers in age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=DR, x='Tumor Size', color='blue')\n",
    "plt.title(\"Tumor Size Box Plot\")\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot(data=DR, x='Survival Months', color='red')\n",
    "plt.title(\"Survival Months Box Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the IQR for both survival months, and the Tumor Size Features:\n",
    "\n",
    "# calculate IQR for the tumor size\n",
    "Q1TS = np.percentile(DR[\"Tumor Size\"], 25)\n",
    "Q3TS = np.percentile(DR[\"Tumor Size\"], 75)\n",
    "IQRTS = Q3TS - Q1TS \n",
    "\n",
    "# get upper bound\n",
    "upper_bound = Q3TS + 1.5 * IQRTS\n",
    "outliers_ts = DR[DR[\"Tumor Size\"] > upper_bound]\n",
    "percentile_of_upper_bound_ts = (DR[\"Tumor Size\"] > upper_bound).mean() * 100\n",
    "print(\"Tumor Size IQR info:\")\n",
    "print(f\"Percentile of upper bound: {percentile_of_upper_bound_ts}%\")\n",
    "print(f\"Number of outliers: {outliers_ts.shape[0]}\")\n",
    "\n",
    "# calculate IQR for the survival months\n",
    "Q1SM = np.percentile(DR[\"Survival Months\"], 25) \n",
    "Q3SM = np.percentile(DR[\"Survival Months\"], 75) \n",
    "IQRSM = Q3SM - Q1SM\n",
    "\n",
    "lower_bound_sm = Q1SM - 1.5 * IQRSM\n",
    "\n",
    "# percentile rank of the lower bound of sm\n",
    "outliers_sm = DR[(DR[\"Survival Months\"] < lower_bound_sm)]\n",
    "percentile_of_lower_bound_sm = (DR[\"Survival Months\"] < lower_bound_sm).mean() * 100\n",
    "print(\"Surival Months IQR info:\")\n",
    "print(f\"Percentile of lower bound: {percentile_of_lower_bound_sm}%\")\n",
    "num_rows_sm = outliers_sm.shape[0]\n",
    "print(f\"Number of outliers: {num_rows_sm}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, both features contain outliers. The Survival months feature has outliers in the 0.83% percentile containing 27 values, and the tumor size box plot shows that there are 176 outliers contained in the upper 5.467% percentile. However, this was just to show that there are outliers that need to be handled. We will find the outliers we want to remove with LOF, by comparing both the Tumor Size and the Survival Months feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first get the features we want to apply LOF outlier removal to:\n",
    "tumor_and_months_df = DR[['Tumor Size', 'Survival Months']].copy()\n",
    "# get the LOF of these two features\n",
    "lof_labels = LocalOutlierFactor(n_neighbors=15, contamination= 0.03).fit_predict(tumor_and_months_df)\n",
    "\n",
    "# modify the new df we created\n",
    "tumor_and_months_df[\"LOF_Score\"] = lof_labels\n",
    "tumor_and_months_df[\"LOF_outlier_val\"] = tumor_and_months_df[\"LOF_Score\"] == -1\n",
    "\n",
    "\n",
    "sns.scatterplot(data=tumor_and_months_df, x='Tumor Size', y ='Survival Months', hue=\"LOF_outlier_val\")\n",
    "plt.legend(title=\"Outlier\")\n",
    "plt.title(\"Survival Months Vs. Tumor Size\")\n",
    "plt.show()\n",
    "\n",
    "# removal of outliers using LOF\n",
    "#tumor_and_months_df_clean = tumor_and_months_df[tumor_and_months_df[\"LOF_Anomaly\"] == False]\n",
    "\n",
    "# create a copy of the dataset to remove the outliers\n",
    "DR2 = DR.copy()\n",
    "\n",
    "DR2[\"LOF_outlier_val\"] = tumor_and_months_df[\"LOF_outlier_val\"]\n",
    "# remove the values\n",
    "DR2 = DR2[DR2[\"LOF_outlier_val\"] == False]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph highlights the outliers found by LOF in orange. LOF parameters are n = 50, and the contamination percentage set to 3.5%. We will deal with these outliers by removing them, and this will be shown later in the empirical study portion of this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Outliers:\n",
    "\n",
    "1. For the comparison of Tumor Size and Survival Months, the outliers found using LOF (as above) will be removed.  \n",
    "2. The Marital Status feature's outlier category, separated, will be combined into the Divorced category.\n",
    "\n",
    "We will expand on these methods of handling the outliers further into the empirical study when the different test/validation sets are created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Analysis: Decision Trees\n",
    "I will be using catboost's decision tree classification methods, instead of scikit's due to catboost's ability to natively process categorical features without any encoding.\\\n",
    "I aim to classify tumor grade using the Decision Tree model trained on patient features.\\\n",
    "Catboost's DecisionTreeClassifier does not have the same parameters as scikit's. Thus, I will be choosing my own parameters for catboost's model.\\\n",
    "I will be employing the 'F1' loss function, since the there are imbalanced classes/category counts.\\\n",
    "The model will iterate 650 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Feature: Tumor Size to Grade ratio\n",
    "Second Feature: Age to Survival Months Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of DR1 (DR) to add the aggregated features to:\n",
    "DR3 = DR.copy()\n",
    "\n",
    "# First feature: Tumor Size to Grade ratio\n",
    "grade_map = {'3':3, '2':2, '1':1,' anaplastic; Grade IV':4}\n",
    "\n",
    "# modify the dataset to add the fixed grade map\n",
    "DR3[\"GradeNumber\"] = DR3[\"Grade\"].map(grade_map)\n",
    "\n",
    "# then add the new feature:\n",
    "DR3['SizeToGradeRatio'] = DR3['GradeNumber'] / DR3['Tumor Size']\n",
    "\n",
    "DR3.drop(columns=['GradeNumber'], inplace=True)\n",
    "\n",
    "# Second Feature: Age to Survival Months Ratio\n",
    "DR3['AgeToSurvivalMonthsRatio'] = DR3['Age'] / DR3['Survival Months']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create the final Dataset copy, DR4 which will include the aggregations, without the outlier values we found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have DR4 which is (DR + aggregation) - outliers\n",
    "DR4 = DR3.copy()\n",
    "\n",
    "# now we need to remove the outliers\n",
    "DR4[\"LOF_outlier_val\"] = tumor_and_months_df[\"LOF_outlier_val\"]\n",
    "DR4 = DR4[DR4[\"LOF_outlier_val\"] == False]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Study: Decision Tree Classification\n",
    "\n",
    "Since we have all 4 versions of the dataset, DR1, DR2, DR3, and DR4\n",
    "we can proceed with 4 cross fold validation and training the decision trees. \n",
    "\n",
    "### DR1:\n",
    "DR1 = D - DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model:\n",
    "device_type = 'GPU' # you can also use CPU if that is to your liking.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the class weights for DR1:\n",
    "\n",
    "# setup the inital catboost model\n",
    "model = CatBoostClassifier(iterations=650, depth=6, learning_rate=0.1, task_type=device_type, verbose=50, loss_function='F1', eval_metric='Accuracy')\n",
    "\n",
    "# Beginning with DR1, get the independent and dependent vars to train for.\n",
    "x_train = DR.drop(columns=['Grade'])\n",
    "y_train = DR['Grade']\n",
    "\n",
    "# get the class weights to improve the catboost classifier, since not everything is equal\n",
    "classes = np.unique(y_train)\n",
    "# compute the weights\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "\n",
    "class_weights = dict(zip(classes, weights))\n",
    "# the weight values are good, but the grade IV has too much weight, thus I will scale it down by 5.\n",
    "class_weights[' anaplastic; Grade IV'] /= 5\n",
    "\n",
    "# Now kFold()\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "<ul>\n",
    "<li>\n",
    "<a href=\"https://catboost.ai/docs/en/concepts/python-usages-examples\">CatBoostClassifier Docs</a>\n",
    "</li>\n",
    "<li>\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\">Sci Kit KFold Docs</a>\n",
    "</li>\n",
    "<li>\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\">Sci Kit LOF Docs</a>\n",
    "</li>\n",
    "<li>\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\">Train Test Split Sci Kit Function Docs</a>\n",
    "</li>\n",
    "<li>\n",
    "<a href=\"https://stackoverflow.com/questions/57565510/usage-of-class-weights-in-catboostclassifier\">Getting weights for CatBoostClassifier</a>\n",
    "</li>\n",
    "<li>\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html\">Sci Kit Compute Class Weights Function Docs</a>\n",
    "</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
